{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmonson1/walkthroughs/blob/main/August_2023_Monthly_Algorithmic_Challenge_First_Unique_Character.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monthly Algorithmic Challenge (August 2023): First Unique Character\n",
        "\n",
        "This post is the second in the sequence of monthly mechanistic interpretability challenges. They are designed in the spirit of [Stephen Casper's challenges](https://www.lesswrong.com/posts/KSHqLzQscwJnv44T8/eis-vii-a-challenge-for-mechanists), but with the more specific aim of working well in the context of the rest of the ARENA material, and helping people put into practice all the things they've learned so far.\n",
        "\n",
        "If you prefer, you can access the Streamlit page [here](https://arena-ch1-transformers.streamlit.app/Monthly_Algorithmic_Problems).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/writer.png\" width=\"350\">"
      ],
      "metadata": {
        "id": "kP1uuM5cat75"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrRgKTEvagv7"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6QECdbLagv-",
        "outputId": "0e32c7f2-6b09-4815-85fa-4d7405325eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m41.0/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m786.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n",
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.2.20-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (1.23.5)\n",
            "Collecting typeguard>=2.13.3 (from jaxtyping)\n",
            "  Downloading typeguard-4.1.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (4.7.1)\n",
            "Installing collected packages: typeguard, jaxtyping\n",
            "Successfully installed jaxtyping-0.2.20 typeguard-4.1.0\n",
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-1.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.6.1)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.20)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.5.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.65.0)\n",
            "Collecting transformers>=4.25.1 (from transformer_lens)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard<4.0.0,>=3.0.2 (from transformer_lens)\n",
            "  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
            "Collecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.31.0)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->transformer_lens)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.25.1->transformer_lens)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=5f575ccccf135af1e3a95e43dc658508520c54bad278be3a7d4c64581ee82729\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, safetensors, pathtools, xxhash, typeguard, smmap, setproctitle, sentry-sdk, fancy-einsum, docker-pycreds, dill, multiprocess, huggingface-hub, gitdb, transformers, GitPython, wandb, datasets, transformer_lens\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.1.0\n",
            "    Uninstalling typeguard-4.1.0:\n",
            "      Successfully uninstalled typeguard-4.1.0\n",
            "Successfully installed GitPython-3.1.32 datasets-2.14.4 dill-0.3.7 docker-pycreds-0.4.0 fancy-einsum-0.0.3 gitdb-4.0.10 huggingface-hub-0.16.4 multiprocess-0.70.15 pathtools-0.1.2 safetensors-0.3.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformer_lens-1.4.0 transformers-4.31.0 typeguard-3.0.2 wandb-0.15.8 xxhash-3.3.0\n",
            "Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
            "  Cloning https://github.com/callummcdougall/CircuitsVis.git to /tmp/pip-req-build-bdb29lcc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /tmp/pip-req-build-bdb29lcc\n",
            "  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit cc216772a66af819ff3a77038e53134f3e073af4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata<6.0.0,>=5.1.0 (from circuitsvis==0.0.0)\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from circuitsvis==0.0.0) (1.23.5)\n",
            "Requirement already satisfied: torch<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis==0.0.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.16.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3.0,>=2.0->circuitsvis==0.0.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3.0,>=2.0->circuitsvis==0.0.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0,>=2.0->circuitsvis==0.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0,>=2.0->circuitsvis==0.0.0) (1.3.0)\n",
            "Building wheels for collected packages: circuitsvis\n",
            "  Building wheel for circuitsvis (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for circuitsvis: filename=circuitsvis-0.0.0-py3-none-any.whl size=1808565 sha256=00f6ea06383f60ca2f217e993df066f72a161cc8394eab833ee3e1580b220973\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xsxtd0kt/wheels/86/be/ad/78078aba9344d200aad61b63d35cdaecdec160212f039eed74\n",
            "Successfully built circuitsvis\n",
            "Installing collected packages: importlib-metadata, circuitsvis\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.4\n",
            "    Uninstalling importlib-metadata-4.6.4:\n",
            "      Successfully uninstalled importlib-metadata-4.6.4\n",
            "Successfully installed circuitsvis-0.0.0 importlib-metadata-5.2.0\n",
            "--2023-08-08 17:32:07--  https://github.com/callummcdougall/ARENA_2.0/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/callummcdougall/ARENA_2.0/zip/refs/heads/main [following]\n",
            "--2023-08-08 17:32:07--  https://codeload.github.com/callummcdougall/ARENA_2.0/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 20.205.243.165\n",
            "Connecting to codeload.github.com (codeload.github.com)|20.205.243.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘main.zip’\n",
            "\n",
            "main.zip                [            <=>     ]  28.40M  9.29MB/s    in 3.1s    \n",
            "\n",
            "2023-08-08 17:32:11 (9.29 MB/s) - ‘main.zip’ saved [29778278]\n",
            "\n",
            "Archive:  /content/main.zip\n",
            "cb8d436714e2410980d40c235794fca55b5cb490\n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/\n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/\n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/dataset.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/first_unique_char_model.pt  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/model.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/training.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/august23_unique_char/training_model.ipynb  \n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/dataset.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/model.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/palindrome_classifier.pt  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/training.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes/training_model.ipynb  \n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/part1_transformer_from_scratch/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part1_transformer_from_scratch/solutions.py  \n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/part2_intro_to_mech_interp/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part2_intro_to_mech_interp/solutions.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part2_intro_to_mech_interp/tests.py  \n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/ioi_circuit_extraction.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/ioi_dataset.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/solutions.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part3_indirect_object_identification/tests.py  \n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/brackets_data.json  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/brackets_datasets.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/brackets_model_state_dict.pt  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/solutions.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part4_interp_on_algorithmic_model/tests.py  \n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/part5_grokking_and_modular_arithmetic/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part5_grokking_and_modular_arithmetic/my_utils.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part5_grokking_and_modular_arithmetic/solutions.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part5_grokking_and_modular_arithmetic/tests.py  \n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/part6_othellogpt/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part6_othellogpt/solutions.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part6_othellogpt/tests.py  \n",
            "   creating: ARENA_2.0-main/chapter1_transformers/exercises/part7_toy_models_of_superposition/\n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part7_toy_models_of_superposition/solutions.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part7_toy_models_of_superposition/tests.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/part7_toy_models_of_superposition/utils.py  \n",
            "  inflating: ARENA_2.0-main/chapter1_transformers/exercises/plotly_utils.py  \n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import os; os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
        "import sys\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Install packages\n",
        "    %pip install einops\n",
        "    %pip install jaxtyping\n",
        "    %pip install transformer_lens\n",
        "    %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "\n",
        "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
        "    import os, sys\n",
        "    if not os.path.exists(\"chapter1_transformers\"):\n",
        "        !curl -o /content/main.zip https://codeload.github.com/callummcdougall/ARENA_2.0/zip/refs/heads/main\n",
        "        !unzip /content/main.zip 'ARENA_2.0-main/chapter1_transformers/exercises/*'\n",
        "        sys.path.append(\"/content/ARENA_2.0-main/chapter1_transformers/exercises\")\n",
        "        os.remove(\"/content/main.zip\")\n",
        "        os.rename(\"ARENA_2.0-main/chapter1_transformers\", \"chapter1_transformers\")\n",
        "        os.rmdir(\"ARENA_2.0-main\")\n",
        "        os.chdir(\"chapter1_transformers/exercises\")\n",
        "else:\n",
        "    from IPython import get_ipython\n",
        "    ipython = get_ipython()\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyvl4hcUagv_"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "from pathlib import Path\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = r\"chapter1_transformers\"\n",
        "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
        "section_dir = exercises_dir / \"monthly_algorithmic_problems\" / \"august23_unique_char\"\n",
        "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
        "\n",
        "from monthly_algorithmic_problems.august23_unique_char.dataset import UniqueCharDataset\n",
        "from monthly_algorithmic_problems.august23_unique_char.model import create_model\n",
        "from plotly_utils import hist, bar, imshow\n",
        "\n",
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLWv5DqwagwA"
      },
      "source": [
        "\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "The following ARENA material should be considered essential:\n",
        "\n",
        "* **[1.1] Transformer from scratch** (sections 1-3)\n",
        "* **[1.2] Intro to Mech Interp** (sections 1-3)\n",
        "\n",
        "The following material isn't essential, but is very strongly recommended:\n",
        "\n",
        "* **[1.2] Intro to Mech Interp** (section 4)\n",
        "* **[1.4] Balanced Bracket Classifier** (all sections)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Difficulty\n",
        "\n",
        "This problem is a step up in difficulty to the July problem. The algorithmic problem is of a similar flavour, and the model architecture is very similar (the main difference is that this model has 3 attention heads per layer, instead of 2)."
      ],
      "metadata": {
        "id": "D-K5co-sHdtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Motivation\n",
        "\n",
        "Neel Nanda's post [200 COP in MI: Interpreting Algorithmic Problems](https://www.lesswrong.com/posts/ejtFsvyhRkMofKAFy/200-cop-in-mi-interpreting-algorithmic-problems) does a good job explaining the motivation behind solving algorithmic problems such as these. I'd strongly recommend reading the whole post, because it also gives some high-level advice for approaching such problems.\n",
        "\n",
        "The main purpose of these challenges isn't to break new ground in mech interp, rather they're designed to help you practice using & develop better understanding for standard MI tools (e.g. interpreting attention, direct logit attribution), and more generally working with libraries like TransformerLens.\n",
        "\n",
        "Also, they're hopefully pretty fun, because why shouldn't we have some fun while we're learning?"
      ],
      "metadata": {
        "id": "uR93nUHLaxHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistics\n",
        "\n",
        "The solution to this problem will be published on this page in the first few days of September, at the same time as the next problem in the sequence. There will also be an associated LessWrong post.\n",
        "\n",
        "If you try to interpret this model, you can send your attempt in any of the following formats:\n",
        "\n",
        "* Colab notebook,\n",
        "* GitHub repo (e.g. with ipynb or markdown file explaining results),\n",
        "* Google Doc (with screenshots and explanations),\n",
        "* or any other sensible format.\n",
        "\n",
        "You can send your attempt to me (Callum McDougall) via any of the following methods:\n",
        "\n",
        "* The [Slack group](https://join.slack.com/t/arena-la82367/shared_invite/zt-1uvoagohe-JUv9xB7Vr143pdx1UBPrzQ), via a direct message to me\n",
        "* My personal email: `cal.s.mcdougall@gmail.com`\n",
        "* LessWrong message ([here](https://www.lesswrong.com/users/themcdouglas) is my user)\n",
        "\n",
        "**I'll feature the names of everyone who sends me a solution on this website, and also give a shout out to the best solutions.** It's possible that future challenges will also feature a monetary prize, but this is not guaranteed.\n",
        "\n",
        "Please don't discuss specific things you've found about this model until the challenge is over (although you can discuss general strategies and techniques, and you're also welcome to work in a group if you'd like). The deadline for this problem will be the end of this month, i.e. 31st August."
      ],
      "metadata": {
        "id": "6_GtzHFIaydj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What counts as a solution?\n",
        "\n",
        "Going through the solutions for the previous problem in the sequence (July: Palindromes) as well as the exercises in **[1.4] Balanced Bracket Classifier** should give you a good idea of what I'm looking for. In particular, I'd expect you to:\n",
        "\n",
        "* Describe a mechanism for how the model solves the task, in the form of the QK and OV circuits of various attention heads (and possibly any other mechanisms the model uses, e.g. the direct path, or nonlinear effects from layernorm),\n",
        "* Provide evidence for your mechanism, e.g. with tools like attention plots, targeted ablation / patching, or direct logit attribution.\n",
        "* (Optional) Include additional detail, e.g. identifying the subspaces that the model uses for certain forms of information transmission, or using your understanding of the model's behaviour to construct adversarial examples."
      ],
      "metadata": {
        "id": "cUO985pZaz7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task & Dataset\n",
        "\n",
        "The algorithmic task is as follows: the model is presented with a sequence of characters, and for each character it has to correctly identify the first character in the sequence (up to and including the current character) which is unique up to that point.\n",
        "\n",
        "The null character `\"?\"` has two purposes:\n",
        "\n",
        "* In the input, it's used as the start character (because it's often helpful for interp to have a constant start character, to act as a \"rest position\").\n",
        "* In the output, it's also used as the start character, **and** to represent the classification \"no unique character exists\".\n",
        "\n",
        "Here is an example of what this dataset looks like:\n",
        "\n",
        "```python\n",
        "dataset = UniqueCharDataset(size=2, vocab=list(\"abc\"), seq_len=6, seed=42)\n",
        "\n",
        "for seq, first_unique_char_seq in zip(dataset.str_toks, dataset.str_tok_labels):\n",
        "    print(f\"Seq = {''.join(seq)}, Target = {''.join(first_unique_char_seq)}\")\n",
        "```\n",
        "\n",
        "<div style='font-family:monospace;'>\n",
        "Seq = ?acbba, Target = ?aaaac<br>\n",
        "Seq = ?cbcbc, Target = ?ccb??\n",
        "</div><br>\n",
        "\n",
        "Explanation:\n",
        "\n",
        "1. In the first sequence, `\"a\"` is unique in the prefix substring `\"acbb\"`, but it repeats at the 5th sequence position, meaning the final target character is `\"c\"` (which appears second in the sequence).\n",
        "2. In the second sequence, `\"c\"` is unique in the prefix substring `\"cb\"`, then it repeats so `\"b\"` is the new first unique token, and for the last 2 positions there are no unique characters (since both `\"b\"` and `\"c\"` have been repeated) so the correct classification is `\"?\"` (the \"null character\").\n",
        "\n",
        "The relevant files can be found in local storage (after you run the setup code at the top of this notebook), at:\n",
        "\n",
        "```\n",
        "chapter1_transformers/\n",
        "└── exercises/\n",
        "    └── monthly_algorithmic_problems/\n",
        "        └── august23_unique_char/\n",
        "            ├── model.py               # code to create the model\n",
        "            ├── dataset.py             # code to define the dataset\n",
        "            ├── training.py            # code to training the model\n",
        "            └── training_model.ipynb   # actual training script\n",
        "```\n",
        "\n",
        "We've given you the class `UniqueCharDataset` to store your data, as you can see above. You can slice this object to get batches of tokens and labels (e.g. `dataset[:5]` returns a length-2 tuple, containing the 2D tensors representing the tokens and correct labels respectively). You can also use `dataset.toks` or `dataset.labels` to access these tensors directly, or `dataset.str_toks` and `dataset.str_tok_labels` to get the string representations of the tokens and labels (like we did in the code above)."
      ],
      "metadata": {
        "id": "bQgyYCaVawF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Our model was trained by minimising cross-entropy loss between its predictions and the true labels, at every sequence position simultaneously (including the zeroth sequence position, which is trivial because the input and target are both always `\"?\"`). You can inspect the notebook `training_model.ipynb` to see how it was trained. I used the version of the model which achieved highest accuracy over 40 epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "2EukMwuOa8Eh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPNbvzmoagwB"
      },
      "source": [
        "The model is is a 2-layer transformer with 3 attention heads, and causal attention. It includes layernorm, but no MLP layers. You can load it in as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVocPt-sagwB"
      },
      "outputs": [],
      "source": [
        "filename = section_dir / \"first_unique_char_model.pt\"\n",
        "\n",
        "model = create_model(\n",
        "    seq_len=20,\n",
        "    vocab=list(\"abcdefghij\"),\n",
        "    seed=42,\n",
        "    d_model=42,\n",
        "    d_head=14,\n",
        "    n_layers=2,\n",
        "    n_heads=3,\n",
        "    normalization_type=\"LN\",\n",
        "    d_mlp=None # attn-only model\n",
        ")\n",
        "\n",
        "state_dict = t.load(filename)\n",
        "\n",
        "state_dict = model.center_writing_weights(t.load(filename))\n",
        "state_dict = model.center_unembed(state_dict)\n",
        "state_dict = model.fold_layer_norm(state_dict)\n",
        "state_dict = model.fold_value_biases(state_dict)\n",
        "model.load_state_dict(state_dict, strict=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiZFDAzFagwC"
      },
      "source": [
        "The code to process the state dictionary is a bit messy, but it's necessary to make sure the model is easy to work with. For instance, if you inspect the model's parameters, you'll see that `model.ln_final.w` is a vector of 1s, and `model.ln_final.b` is a vector of 0s (because the weight and bias have been folded into the unembedding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SYMGt0sagwC",
        "outputId": "53295173-507a-4db0-fb39-482e35a26c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ln_final weight:  Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n",
            "\n",
            "ln_final, bias:  Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(\"ln_final weight: \", model.ln_final.w)\n",
        "print(\"\\nln_final, bias: \", model.ln_final.b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE6-KTCEagwC"
      },
      "source": [
        "<details>\n",
        "<summary>Aside - the other weight processing parameters</summary>\n",
        "\n",
        "Here's some more code to verify that our weights processing worked, in other words:\n",
        "\n",
        "* The unembedding matrix has mean zero over both its input dimension (`d_model`) and output dimension (`d_vocab`)\n",
        "* All writing weights (i.e. `b_O`, `W_O`, and both embeddings) have mean zero over their output dimension (`d_model`)\n",
        "* The value biases `b_V` are zero (because these can just be folded into the output biases `b_O`)\n",
        "\n",
        "```python\n",
        "W_U_mean_over_input = einops.reduce(model.W_U, \"d_model d_vocab -> d_model\", \"mean\")\n",
        "t.testing.assert_close(W_U_mean_over_input, t.zeros_like(W_U_mean_over_input))\n",
        "\n",
        "W_U_mean_over_output = einops.reduce(model.W_U, \"d_model d_vocab -> d_vocab\", \"mean\")\n",
        "t.testing.assert_close(W_U_mean_over_output, t.zeros_like(W_U_mean_over_output))\n",
        "\n",
        "W_O_mean_over_output = einops.reduce(model.W_O, \"layer head d_head d_model -> layer head d_head\", \"mean\")\n",
        "t.testing.assert_close(W_O_mean_over_output, t.zeros_like(W_O_mean_over_output))\n",
        "\n",
        "b_O_mean_over_output = einops.reduce(model.b_O, \"layer d_model -> layer\", \"mean\")\n",
        "t.testing.assert_close(b_O_mean_over_output, t.zeros_like(b_O_mean_over_output))\n",
        "\n",
        "W_E_mean_over_output = einops.reduce(model.W_E, \"token d_model -> token\", \"mean\")\n",
        "t.testing.assert_close(W_E_mean_over_output, t.zeros_like(W_E_mean_over_output))\n",
        "\n",
        "W_pos_mean_over_output = einops.reduce(model.W_pos, \"position d_model -> position\", \"mean\")\n",
        "t.testing.assert_close(W_pos_mean_over_output, t.zeros_like(W_pos_mean_over_output))\n",
        "\n",
        "b_V = model.b_V\n",
        "t.testing.assert_close(b_V, t.zeros_like(b_V))\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3noWR5QRagwD"
      },
      "source": [
        "The model's output is a logit tensor, of shape `(batch_size, seq_len, d_vocab+1)`. The `[i, j, :]`-th element of this tensor is the logit distribution for the label at position `j` in the `i`-th sequence in the batch. The first `d_vocab` elements of this tensor correspond to the elements in the vocabulary, and the last element corresponds to the null character `\"?\"` (which is not in the input vocab).\n",
        "\n",
        "A demonstration of the model working:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLtoK5nXagwD",
        "outputId": "5f78ad09-c4f3-4327-a081-f00bc6054d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cross entropy loss: 0.017\n",
            "Average probability on correct label: 0.988\n",
            "Min probability on correct label: 0.001\n"
          ]
        }
      ],
      "source": [
        "dataset = UniqueCharDataset(size=1000, vocab=list(\"abcdefghij\"), seq_len=20, seed=42)\n",
        "\n",
        "logits, cache = model.run_with_cache(dataset.toks)\n",
        "\n",
        "logprobs = logits.log_softmax(-1) # [batch seq_len d_vocab]\n",
        "probs = logprobs.softmax(-1) # [batch seq_len d_vocab]\n",
        "\n",
        "batch_size, seq_len = dataset.toks.shape\n",
        "logprobs_correct = logprobs[t.arange(batch_size)[:, None], t.arange(seq_len)[None, :], dataset.labels] # [batch seq_len]\n",
        "probs_correct = probs[t.arange(batch_size)[:, None], t.arange(seq_len)[None, :], dataset.labels] # [batch seq_len]\n",
        "\n",
        "avg_cross_entropy_loss = -logprobs_correct.mean().item()\n",
        "avg_correct_prob = probs_correct.mean().item()\n",
        "min_correct_prob = probs_correct.min().item()\n",
        "\n",
        "print(f\"Average cross entropy loss: {avg_cross_entropy_loss:.3f}\")\n",
        "print(f\"Average probability on correct label: {avg_correct_prob:.3f}\")\n",
        "print(f\"Min probability on correct label: {min_correct_prob:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And a visualisation of its probability output for a single sequence:"
      ],
      "metadata": {
        "id": "VW8uJNzlH-3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(\n",
        "    probs[0].T,\n",
        "    y=dataset.vocab,\n",
        "    x=[f\"{dataset.str_toks[0][i]}<br>({i})\" for i in range(model.cfg.n_ctx)],\n",
        "    labels={\"x\": \"Token\", \"y\": \"Vocab\"},\n",
        "    xaxis_tickangle=0,\n",
        "    title=\"Sample model probabilities (for batch idx = 0), with correct classification highlighted\",\n",
        "    text=[\n",
        "        [\"〇\" if str_tok == correct_str_tok else \"\" for correct_str_tok in dataset.str_tok_labels[0]]\n",
        "        for str_tok in dataset.vocab\n",
        "    ], # text can be a 2D list of lists, with the same shape as the data\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "FMiPGFraIHjm",
        "outputId": "0ff164f2-aa2f-4a23-cf14-2ece86fb8c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"830017d5-5328-4493-9522-e77bbf97eba1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"830017d5-5328-4493-9522-e77bbf97eba1\")) {                    Plotly.newPlot(                        \"830017d5-5328-4493-9522-e77bbf97eba1\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"?<br>(0)\",\"c<br>(1)\",\"h<br>(2)\",\"g<br>(3)\",\"e<br>(4)\",\"g<br>(5)\",\"f<br>(6)\",\"a<br>(7)\",\"e<br>(8)\",\"a<br>(9)\",\"d<br>(10)\",\"i<br>(11)\",\"e<br>(12)\",\"a<br>(13)\",\"e<br>(14)\",\"b<br>(15)\",\"c<br>(16)\",\"f<br>(17)\",\"f<br>(18)\",\"h<br>(19)\"],\"y\":[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"?\"],\"z\":[[2.405383010828377e-11,6.955629422060401e-17,7.009084856190121e-25,1.819461332314082e-24,1.2306120469781958e-23,2.6746518368622756e-20,2.382831230826671e-22,1.520550931599737e-19,1.2450465293890444e-15,6.634122606407395e-21,2.4080296253955284e-23,9.596573842831597e-21,1.667018768688736e-19,2.598053134994018e-21,7.264905643827988e-24,4.677527071680833e-24,5.710471953941175e-23,1.9483334599891587e-20,4.6780188185390663e-20,9.99554258807045e-12],[3.696474959724583e-05,1.0569252617075861e-20,1.8773614745037244e-23,1.816085283741053e-23,9.600336242343845e-24,6.1811500372967755e-19,6.240964226716438e-21,1.6176951280035638e-23,1.2486833149789175e-18,4.5645976870268705e-18,1.7634300295030298e-19,3.094707302795973e-19,4.282718687942685e-17,1.1891068853655943e-15,1.2599516156959478e-13,3.9759715702095133e-13,1.424395405227527e-13,3.276240606933656e-12,4.746017814860615e-12,0.034129418432712555],[1.675767533533375e-28,1.0,1.0,1.0,1.0,0.9999998807907104,0.9999991655349731,0.9999997615814209,0.9999995231628418,0.9999984502792358,0.9999943971633911,0.9999651908874512,0.9999457597732544,0.9998093247413635,0.9984952211380005,0.9966981410980225,5.1482945843476174e-20,1.892305221303189e-20,8.747732762768787e-19,3.480495785242871e-13],[1.1831470525525134e-10,2.9366629634328633e-16,2.4102922658261673e-25,4.881961854383055e-25,7.734923801743685e-23,1.5169764773777422e-19,1.6738679857193884e-20,6.871031495927844e-23,5.570605959219854e-19,2.1406491266974046e-18,3.336933941311599e-17,2.170362473386824e-15,1.7441619641080618e-14,7.045598913318907e-13,2.3143337940512154e-11,6.219362524984007e-11,1.1606732935875996e-11,1.5059421765162284e-10,1.5001683228987872e-10,0.6060590744018555],[3.399214520971583e-10,1.7682686806572955e-20,1.1763822921564625e-25,1.2326069510990906e-26,4.027072458053643e-15,2.7766230612479015e-13,1.3260111053885887e-13,1.214780907159499e-13,4.700935841115926e-26,1.9577843441849146e-28,9.178815100712201e-30,1.3462324038272462e-29,1.826383247793726e-27,3.0777433254287053e-27,9.457116443426215e-27,1.1971242341012824e-26,3.192920484264873e-27,2.0651212562268172e-23,2.939133628879443e-20,2.1630646163117717e-09],[3.606618292906205e-08,3.1360791030243306e-18,2.181318140426202e-24,6.794730147604744e-26,9.08516034144306e-22,2.661058021761413e-18,8.370805748218583e-17,1.893456656198875e-16,1.9329008608525572e-13,2.4619536010545007e-12,1.6100614765035548e-12,9.581986246121055e-12,5.10777184437039e-11,7.519492806196126e-10,8.76849881592534e-09,1.28598580673156e-08,1.417612249809963e-08,6.876283769700129e-21,2.2689205030015097e-25,1.0604345391056318e-21],[2.9204961898534635e-18,4.774596759477964e-21,1.3288359059553027e-27,3.254012777067544e-24,2.478803620697967e-11,3.3510882456772795e-31,2.976454432607503e-28,1.412357316501612e-23,7.9514546423006815e-25,1.795033268307782e-24,1.144495625828436e-19,1.4241246599614854e-22,1.7958188033754264e-20,2.0462794397233582e-21,1.6400074701291524e-18,1.1126307988542465e-23,1.9886398784725302e-29,1.2954481126623294e-32,2.456381456269244e-30,4.563983166148752e-24],[7.587574124556795e-09,1.2171075756381295e-20,2.0801286407510504e-22,2.9470260951459353e-20,4.4046442049250345e-09,8.373437765385461e-08,8.17665750219021e-07,2.806586962833535e-07,4.882506914327678e-07,1.6017261259548832e-06,5.5796426750021055e-06,3.4767846955219284e-05,5.4218227887758985e-05,0.0001906609395518899,0.0015047487104311585,0.0033019075635820627,1.0,1.0,1.0,8.131183985675416e-13],[4.630033174635173e-07,1.1606258063027076e-24,7.8263568334317e-26,5.7656255980032e-25,6.28758492171659e-23,1.1780073490811617e-18,4.08166909515007e-20,3.4235126928655686e-22,2.9630006588343477e-18,9.138625233036846e-18,6.235467886195836e-19,1.19803890073395e-16,1.2270634269957725e-14,3.105503864047482e-13,1.043615714679813e-11,2.090597313320064e-11,4.710081283332279e-12,4.244449236523451e-11,5.831628235863917e-11,0.3598007559776306],[2.980721447443102e-08,9.197059945304298e-22,2.6583907461514057e-25,9.956138912786943e-26,1.5803843737693256e-23,6.129866907270019e-19,2.0170072222895328e-20,2.4169430909813655e-22,1.321400555833301e-18,1.1365785044098108e-17,2.3728640053485106e-19,8.089492933584175e-19,1.9652420036469623e-17,7.464126858401003e-16,4.706165538108549e-14,1.065762688740252e-14,9.857886568920285e-17,3.976724131454578e-16,7.884169967548819e-16,7.677269309169787e-07],[0.9999624490737915,3.226304787189516e-22,2.515813741914229e-22,1.1623044530052113e-25,6.1328693400218474e-24,2.10803546309597e-18,9.402016358335793e-22,4.115748455497647e-24,8.680811783261715e-19,8.134074866673486e-19,7.69577337970226e-21,3.2244960364189274e-21,6.038400273663626e-18,9.339064992092735e-17,2.5577511706927417e-14,2.8690726459177263e-16,1.4759371758382852e-16,1.164231751166222e-15,2.813237822221499e-15,1.000959673547186e-05]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Token: %{x}<br>Vocab: %{y}<br>color: %{z}<extra></extra>\",\"textfont\":{\"size\":12},\"text\":[[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\\u3007\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\\u3007\",\"\\u3007\",\"\\u3007\",\"\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],[\"\\u3007\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]],\"texttemplate\":\"%{text}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Token\"},\"tickangle\":0},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Vocab\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Sample model probabilities (for batch idx = 0), with correct classification highlighted\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('830017d5-5328-4493-9522-e77bbf97eba1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want some guidance on how to get started, I'd recommend reading the solutions for the July problem - I expect there to be a lot of overlap in the best way to tackle these two problems. You can also reuse some of that code!\n"
      ],
      "metadata": {
        "id": "Zy9RPWurJywI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note - although this model was trained for long enough to get loss close to zero (you can test this for yourself), it's not perfect. There are some weaknesses that the model has which might make it vulnerable to adversarial examples, and I've decided to leave these in. The model is still very good at its intended task, and the main focus of this challenge is on figuring out how it solves the task, not dissecting the situations where it fails. However, you might find that the adversarial examples help you understand the model better.\n"
      ],
      "metadata": {
        "id": "z5KggcQ0dbk-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh9CtuD5agwE"
      },
      "source": [
        "Best of luck! 🎈"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YrRgKTEvagv7"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}